
Modern hierarchical, agglomerative clustering algorithms
Daniel Müllner

arXiv:1109.2378v1 [stat.ML] 12 Sep 2011

This paper presents algorithms for hierarchical, agglomerative clustering which perform most eﬃciently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a “stepwise dendrogram”, a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting eﬃciently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs signiﬁcantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for diﬀerent reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes. Keywords: clustering, hierarchical, agglomerative, partition, linkage

1 Introduction
Hierarchical, agglomerative clustering is an important and well-established technique in unsupervised machine learning. Agglomerative clustering schemes start from the partition of the data set into singleton nodes and merge step by step the current pair of mutually closest nodes into a new node until there is one ﬁnal node left, which comprises the entire data set. Various clustering schemes share this procedure as a common deﬁnition, but diﬀer in the way in which the measure of inter-cluster dissimilarity is updated after each step. The seven most common methods are termed single, complete, average (UPGMA), weighted (WPGMA, McQuitty), Ward, centroid (UPGMC) and median (WPGMC) linkage (see Everitt et al., 2011, Table 4.1). They are implemented in standard numerical and statistical software such as R (R Development Core Team, 2011), MATLAB (The MathWorks, Inc., 2011), Mathematica (Wolfram Research, Inc., 2010), SciPy (Jones et al., 2001). The stepwise, procedural deﬁnition of these clustering methods directly gives a valid but ineﬃcient clustering algorithm. Starting with Gower’s and Ross’s observation (Gower and Ross, 1969) that single linkage clustering is related to the minimum spanning tree of a graph in 1969, several authors have contributed algorithms to reduce the computational complexity of agglomerative clustering, in particular Sibson (1973), Rohlf (1973), Anderberg (1973, page 135), Murtagh (1984), Day and Edelsbrunner (1984, Table 5).

1

Even when software packages do not use the ineﬃcient primitive algorithm (as SciPy (Eads, 2007) and the R (R Development Core Team, 2011) methods hclust and agnes do), the author found that implementations largely use suboptimal algorithms rather than improved methods suggested in theoretical work. This paper is to advance the theory further up to a point where the algorithms can be readily used in the standard setting, and in this way bridge the gap between the theoretical advances that have been made and the existing software implementations, which are widely used in science and industry. The main contributions of this paper are: • We present a new algorithm which is suitable for any distance update scheme and performs signiﬁcantly better than the existing algorithms for the “centroid” and “median” clustering schemes. • We prove the correctness of two algorithms, a single linkage algorithm by Rohlf (1973) and Murtagh’s nearest-neighbor-chain algorithm (Murtagh, 1985, page 86). These proofs were still missing, and we detail why the two proofs are necessary, each for diﬀerent reasons. • These three algorithms (together with an alternative by Sibson, 1973) are the best currently available ones, each for its own subset of agglomerative clustering schemes. We justify this carefully, discussing potential alternatives. The speciﬁc class of clustering algorithms which is dealt with in this paper has been characterized by the acronym SAHN (sequential, agglomerative, hierarchic, nonoverlapping methods) by Sneath and Sokal (1973, § 5.4, 5.5). The procedural deﬁnition (which is given in Figure 1 below) is not the only possibility for a SAHN method, but this method together with the seven common distance update schemes listed above is most widely used. The scope of this paper is contained further by practical considerations: We consider methods here which comply to the input and output requirements of the general-purpose clustering functions in modern standard software: • The input to the algorithm is the list of N 2 pairwise dissimilarities between N points. (We mention extensions to vector data in Section 6.) • The output is a so called stepwise dendrogram (see Section 2.2), in contrast to laxly speciﬁed output structure or weaker notions of (non-stepwise) dendrograms in earlier literature. The ﬁrst item has always been a distinctive characteristic to previous authors since the input format broadly divides into the stored matrix approach (Anderberg, 1973, § 6.2) and the stored data approach (Anderberg, 1973, § 6.3). In contrast, the second condition has not been given attention yet, but we will see that it aﬀects the validity of algorithms. We do not aim to present and compare all available clustering algorithms but build upon the existing knowledge and present only the algorithms which we found the best for the given purpose. For reviews and overviews we refer to Rohlf (1982), Murtagh (1983, 1985), Gordon (1987, §3.1), Jain and Dubes (1988, § 3.2), Day (1996, § 4.2), Hansen and Jaumard (1997). Those facts about alternative algorithms which are necessary to complete the discussion and which are not covered in existing reviews are collected in Section 5. The paper is structured as follows: Section 2 contains the deﬁnitions for input and output data structures as well as speciﬁcations of the distance update formulas and the “primitive” clustering algorithm.

2

Section 3 is the main section of this paper. We present and discuss three algorithms: our own “generic algorithm”, Murtagh’s nearest-neighbor-chain algorithm and Rohlf’s algorithm based on the minimum spanning tree of a graph. We prove the correctness of these algorithms. Section 4 discusses the complexity of the algorithms, both as theoretical, asymptotic complexity in Section 4.1 and by use-case performance experiments in Section 4.2. We conclude this section by recommendations on which algorithm is the best one for each distance update scheme, based on the preceding analysis. Section 5 discusses alternative algorithms, and Section 6 gives a short outlook on extending the context of this paper to vector data instead of dissimilarity input. The paper ends with a brief conclusion in Section 7. The algorithms in this paper have been implemented in C++ by the author and are available with interfaces to the statistical software R and the programming language Python (van Rossum et al.). This implementation is presented elsewhere (Müllner, 2011).

2 Data structures and the algorithmic deﬁnition of SAHN clustering methods
In this section, we recall the common algorithmic (procedural) deﬁnition of the SAHN clustering methods which demarcate the scope of this paper. Before we do so, we concretize the setting further by specifying the input and output data structures for the clustering methods. Especially the output data structure has not been speciﬁcally considered in earlier works, but nowadays there is a de facto standard given by the shared conventions in the most widely used software. Hence, we adopt the setting from practice and specialize our theoretical consideration to the modern standard of the stepwise dendrogram. Later, Section 5 contains an example of how the choice of the output data structure aﬀects the result which algorithms are suitable and/or most eﬃcient.

2.1 Input data structure
The input to the hierarchical clustering algorithms in this paper is always a ﬁnite set together with a dissimilarity index (see Hansen and Jaumard, 1997, § 2.1). Deﬁnition. A dissimilarity index on a set S is a map d : S × S → [0, ∞) which is reﬂexive and symmetric, i.e. we have d(x, x) = 0 and d(x, y ) = d(y, x) for all x, y ∈ S . A metric on S is certainly a dissimilarity index. In the scope of this paper, we call the values of d distances in a synonymous manner to dissimilarities, even though they are not required to fulﬁll the triangle inequalities, and dissimilarities between diﬀerent elements may be zero. If the set S has N elements, a dissimilarity index is given by the N 2 pairwise dissimilarities. Hence, the input size to the clustering algorithms is Θ(N 2 ). Once the primitive clustering algorithm is speciﬁed in Section 2.4, it is easy to see that the hierarchical clustering schemes are sensitive to each input value. More precisely, for every input size N and for every index pair i = j , there are two dissimilarities which diﬀer only at position (i, j ) and which produce diﬀerent output. Hence, all input values must be processed by a clustering algorithm, and therefore the run-time is bounded below by Ω(N 2 ). This bound applies to the general setting when the input is a dissimilarity index. In a diﬀerent setting, the input could also be given as N points in a normed vector space of dimension D (the “stored data approach”, Anderberg, 1973, §6.3). This results in an input size of Θ(N D), so that the lower bound does not apply for clustering of vector data. See

3

Section 6 for a discussion to which extent the algorithms in this paper can be used in the “stored data approach”.

2.2 Output data structures
The output of a hierarchical clustering procedure is traditionally a dendrogram. The term “dendrogram” has been used with three diﬀerent meanings: a mathematical object, a data structure and a graphical representation of the former two. In the course of this section, we deﬁne a data structure and call it stepwise dendrogram. A graphical representation may be drawn from the data in one of several existing fashions. The graphical representation might lose information (e.g. when two merges happen at the same dissimilarity value), and at the same time contain extra information which is not contained in the data itself (like a linear order of the leaves). In the older literature, e.g. Sibson (1973), a dendrogram (this time, as a mathematical object) is rigorously deﬁned as a piecewise constant, right-continuous map D : [0, ∞) → P (S ), where P (S ) denotes the partitions of S , such that • D(s) is always coarser than or equal to D(t) for s > t, • D(s) eventually becomes the one-set partition {S } for large s. A dendrogram in this sense with the additional condition that D(0) is the singleton partition is in one-to-one correspondence to an ultrametric on S (Johnson, 1967, § I). The ultrametric distance between x and y is given by µ(x, y ) = min{s ≥ 0 | x ∼ y in D(s)}. Conversely, the partition at level s ≥ 0 in the dendrogram is given by the equivalence relation x ∼ y ⇔ µ(x, y ) ≤ s. Sibson’s “pointer representation” and “packed representation” (Sibson, 1973, § 4) are examples of data structures which allow the compact representation of a dendrogram or ultrametric. In the current software, however, the output of a hierarchical clustering procedure is a diﬀerent data structure which conveys potentially more information. We call this a stepwise dendrogram. Deﬁnition. Given a ﬁnite set S0 with cardinality N = |S0 |, a stepwise dendrogram is a list of N − 1 triples (ai , bi , δi ) (i = 0, . . . , N − 2) such that δi ∈ [0, ∞) and ai , bi ∈ Si , where Si+1 is recursively deﬁned as (Si \ {ai , bi }) ∪ ni and ni ∈ / S \ {ai , bi } is a label for a new node. This has the following interpretation: The set S0 are the initial data points. In each step, ni is the new node which is formed by joining the nodes ai and bi at the distance δi . The order of the nodes within each pair (ai , bi ) does not matter. The procedure contains N − 1 steps, so that the ﬁnal state is a single node which contains all N initial nodes. (The mathematical object behind this data structure is a sequence of N + 1 distinct, nested partitions from the singleton partition to the one-set partition, together with a nonnegative real number for each partition. We do not need this abstract point of view here, though.) The identity of the new labels ni is not part of the data structure; instead it is assumed that they are generated according to some rule which is part of the speciﬁc data format convention. In view of this, it is customary to label the initial data points and the new nodes by integers. For example, the following schemes are used in software: • R convention: S0 := (−1, . . . , −N ), new nodes: (1, . . . , N − 1) • SciPy convention: S0 := (0, . . . , N − 1), new nodes: (N, . . . , 2N − 2) • MATLAB convention: S0 := (1, . . . , N ), new nodes: (N + 1, . . . , 2N − 1)

4

We regard a stepwise dendrogram both as an ((N − 1) × 3)-matrix or as a list of triples, whichever is more convenient in a given situation. If the sequence (δi ) in Section 2.2 is non-decreasing, one says that the stepwise dendrogram does not contain inversions, otherwise it does. In contrast to the ﬁrst notion of a dendrogram above, a stepwise dendrogram can take inversions into account, which an ordinary dendrogram cannot. Moreover, if more than two nodes are joined at the same distance, the order of the merging steps does matter in a stepwise dendrogram. Consider e.g. the following data sets with three points: (A)
2.0

x0 •
3.0

(B )
2.0 2.0

x1 •
3.0

(C )
2.0 2. 0

x2 •
3.0

2.0

x1 The array

•

• x2

x2

•

• x0

x0

•

• x1

0, 1, 2.0 2, 3, 2.0 is a valid output (SciPy conventions) for single linkage clustering on the data sets (A) and (B ) but not for (C ). Even more, there is no stepwise dendrogram which is valid for all three data sets simultaneously. On the other hand, the non-stepwise single linkage dendrogram is the same in all cases: D(s) = {x0 }, {x1 }, {x2 } {x0 , x1 , x2 } if s Figure 1 Algorithmic deﬁnition of a hierarchical clustering scheme. procedure Primitive_clustering(S, d) N ← |S | L ← [] size [x] ← 1 for all x ∈ S for i ← 0, . . . , N − 2 do (a, b) ← argmin(S ×S )\∆ d Append (a, b, d[a, b]) to L. 8: S ← S \ {a, b} 9: Create a new node label n ∈ / S. 10: Update d with the information
1: 2: 3: 4: 5: 6: 7:

S : node labels, d: pairwise dissimilarities Number of input nodes Output list

d[n, x] = d[x, n] = Formula(d[a, x], d[b, x], d[a, b], size [a], size [b], size [x]) for all x ∈ S . size [n] ← size [a] + size [b] S ← S ∪ { n} end for return L the stepwise dendrogram, an ((N − 1) × 3)-matrix 15: end procedure (As usual, ∆ denotes the diagonal in the Cartesian product S × S .)
11: 12: 13: 14:

2.4 The primitive clustering algorithm
The solution that we expect from a hierarchical clustering algorithm is deﬁned procedurally. All algorithms in this paper are measured against the primitive algorithm in Figure 1. We state it in a detailed form to point out exactly which information about the clusters is maintained: the pairwise dissimilarities and the number of elements in each cluster. The function Formula in line 10 is the distance update formula, which returns the distance from a node x to the newly formed node a ∪ b in terms of the dissimilarities between clusters a, b and x and their sizes. The table in Figure 2 lists the formulas for the common distance update methods. For ﬁve of the seven formulas, the distance between clusters does not depend on the order which the clusters were formed by merging. In this case, we also state closed, non-iterative formulas for the cluster dissimilarities in the third row in Figure 2. The distances in the “weighted” and the “median” update scheme depend on the order, so we cannot give noniterative formulas. The “centroid” and “median” formulas can produce inversions in the stepwise dendrograms; the other ﬁve methods cannot. This can be checked easily: The sequence of dissimilarities at which clusters are merged in Figure 1 cannot decrease if the following condition is fulﬁlled for all disjoint subsets I, J, K ⊂ S0 : d(I, J ) ≤ min{d(I, K ), d(J, K )} ⇒ d(I, J ) ≤ d(I ∪ J, K )

On the other hand, conﬁgurations with inversion in the “centroid” and “median” schemes can be easily produced, e.g. three points near the vertices of an equilateral triangle in R2 . The primitive algorithm takes Θ(N 3 ) time since in the i-th iteration of N − 1 in total, all N −1−i ∈ Θ(i2 ) pairwise distances between the N − i nodes in S are searched. 2 Note that the stepwise dendrogram from a clustering problem (S, d) is not always uniquely deﬁned, since the minimum in line 6 of the algorithm might be attained for several index

6

Figure 2 Agglomerative clustering schemes. Name single complete average Distance update formula Formula for d(I ∪ J, K ) min(d(I, K ), d(J, K )) max(d(I, K ), d(J, K )) nI d(I, K ) + nJ d(J, K ) nI + nJ d(I, K ) + d(J, K ) 2 (nI + nK )d(I, K ) + (nJ + nK )d(J, K ) − nK d(I, J ) nI + nJ + nK nI d(I, K ) + nJ d(J, K ) nI nJ d(I, J ) − nI + nJ (nI + nJ )2 d(I, K ) d(J, K ) d(I, J ) + − 2 2 4 2|A||B | · cA − cB |A| + |B | cA − cB
2 2

Cluster dissimilarity between clusters A and B
a∈A,b∈B a∈A,b∈B

min

d[a, b]

max d[a, b] d[a, b]
a∈A b∈B

1 |A||B |

weighted Ward

centroid

median

wA − w B

2

Legend: Let I, J be two clusters joined into a new cluster, and let K be any other cluster. Denote by nI , nJ and nK the sizes of (i.e. number of elements in) clusters I, J, K , respectively. The update formulas for the “Ward”, “centroid” and “median” methods assume that the input points are given as vectors in Euclidean space with the Euclidean distance as dissimilarity measure. The expression cX denotes the centroid of a cluster X . The point wX is deﬁned iteratively and depends on the clustering steps: If the cluster L is formed by joining I and J , 1 we deﬁne wL as the midpoint 2 (wI + wJ ). All these formulas can be subsumed (for squared Euclidean distances in the three latter cases) under a single formula d(I ∪ J, K ) := αI d(I, K ) + αJ d(J, K ) + βd(I, J ) + γ |d(I, K ) − d(J, K )|, where the coeﬃcients αI , αJ , β may depend on the number of elements in the clusters I , J 1 , β = 0, γ = − 1 and K . For example, αI = αJ = 2 2 gives the single linkage formula. All clustering methods which use this formula are combined under the name “ﬂexible” in this paper, as introduced by Lance and Williams (1967). References: Lance and Williams (1967), Kaufman and Rousseeuw (1990, §5.5.1)

7

pairs. We consider every possible output of Primitive_clustering under any choices of minima as a valid output.

3 Algorithms
In the main section of this paper, we present three algorithms which are the most eﬃcient ones for the task of SAHN clustering with the stored matrix approach. Two of the algorithms were described previously: The nearest-neighbor chain (“NN-chain”) algorithm by Murtagh (1985, page 86), and an algorithm by Rohlf (1973), which we call “MST-algorithm” here since it is based on Prim’s algorithm for the minimum spanning tree of a graph. Both algorithms were presented by the respective authors, but for diﬀerent reasons each one still lacks a correctness proof. Sections 3.2 and 3.3 state the algorithms in a way which is suitable for modern standard input and output structures and supply the proofs of correctness. The third algorithm in Section 3.1 is a new development based on Anderberg’s idea to maintain a list of nearest neighbors for each node Anderberg (1973, pages 135–136). While we do not show that the worst-case behavior of our algorithm is better than the O(N 3 ) worstcase complexity of Anderberg’s algorithm, the new algorithm is for all inputs at least equally fast, and we show by experiments in Section 4.2 that the new algorithm is considerably faster in practice since it cures Anderberg’s algorithm from its worst-case behavior at random input. As we saw in the last section, the solution to a hierarchical clustering task does not have a simple, self-contained speciﬁcation but is deﬁned as the outcome of the “primitive” clustering algorithm. The situation is complicated by the fact that the primitive clustering algorithm itself is not completely speciﬁed: if a minimum inside the algorithm is attained at more than one place, a choice must be made. We do not require that ties are broken in a speciﬁc way; instead we consider any output of the primitive algorithm under any choices as a valid solution. Each of the “advanced” algorithms is considered correct if it always returns one of the possible outputs of the primitive algorithm.

3.1 The generic clustering algorithm
The most generally usable algorithm is described in this section. We call it Generic_ linkage since it can be used with any distance update formula. It is the only algorithm among the three in this paper which can deal with inversions in the dendrogram. Consequentially, the “centroid” and “median” methods must use this algorithm. The algorithm is presented in Figure 3. It is a sophistication of the primitive clustering algorithm and of Anderberg’s approach (Anderberg, 1973, pages 135–136). Brieﬂy, candidates for nearest neighbors of clusters are cached in a priority queue to speed up the repeated minimum searches in line 6 of Primitive_clustering. For the pseudocode in Figure 3, we assume that the set S are integer indices from 0 to N − 1. This is the way in which it may be done in an implementation, and it makes the description easier than for an abstract index set S . In particular, we rely on an order of the index set (see e.g. line 6: the index ranges over all y > x). There are two levels of sophistication from the primitive clustering algorithm to our generic clustering algorithm. In a ﬁrst step, one can maintain a list of nearest neighbors for each cluster. For the sake of speed, it is enough to search for the nearest neighbors of a node x only among the nodes with higher index y > x. Since the dissimilarity index is symmetric, this list will still contain a pair of closest nodes. The list of nearest neighbors speeds up the global i−1 minimum search in the i-th step from N − comparisons to N − i − 1 comparisons at the 2 beginning of each iteration. However, the list of nearest neighbors must be maintained: if the

8

Figure 3 The generic clustering algorithm.
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43:

procedure Generic_linkage(N, d) N : input size, d: pairwise dissimilarities S ← (0, . . . , N − 1) L ← [] Output list size [x] ← 1 for all x ∈ S for x in S \ {N − 1} do Generate the list of nearest neighbors. n _nghbr [x] ← argminy>x d[x, y ] mindist [x] ← d[x, n _nghbr [x]] end for Q ← (priority queue of indices in S \ {N − 1}, keys are in mindist ) for i ← 1, . . . , N − 1 do Main loop. a ← (minimal element of Q) b ← n _nghbr [a] δ ← mindist [a] while δ = d[a, b] do Recalculation of nearest neighbors, if necessary. n _nghbr [a] ← argminx>a d[a, x] Update mindist and Q with (a, d[a, n _nghbr [a]]) a ← (minimal element of Q) b ← n _nghbr [a] δ ← mindist [a] end while Remove the minimal element a from Q. Append (a, b, δ ) to L. Merge the pairs of nearest nodes. size [b] ← size [a] + size [b] Re-use b as the index for the new node. S ← S \ {a} for x in S \ {b} do Update the distance matrix. d[x, b] ← d[b, x] ← Formula(d[a, x], d[b, x], d[a, b], size [a], size [b], size [x]) end for for x in S such that x b d[b, x] Update mindist and Q with (b, d[b, n _nghbr [b]]) end for return L The stepwise dendrogram, an ((N − 1) × 3)-matrix. end procedure

9

nearest neighbor of a node x is one of the clusters a, b which are joined, then it is sometimes necessary to search again for the nearest neighbor of x among all nodes y > x. Altogether, this reduces the best-case complexity of the clustering algorithm from Θ(N 3 ) to Θ(N 2 ), while the worst case complexity remains O(N 3 ). This is the method that Anderberg suggested in (Anderberg, 1973, pages 135–136). On a second level, one can try to avoid or delay the nearest neighbor searches as long as possible. Here is what the algorithm Generic_linkage does: It maintains a list n _nghbr of candidates for nearest neighbors, together with a list mindist of lower bounds for the distance to the true nearest neighbor. If the distance d[x, n _nghbr [x]] is equal to mindist [x], we know that we have the true nearest neighbor, since we found a realization of the lower bound; otherwise the algorithm must search for the nearest neighbor of x again. To further speed up the minimum searches, we also make the array mindist into a priority queue, so that the current minimum can be found quickly. We require a priority queue Q with a minimal set of methods as in the list below. This can be implemented conveniently by a binary heap (see Cormen et al., 2009, Chapter 6). We state the complexity of each operation by the complexity for a binary heap. • Queue(v ): Generate a new queue from a vector v of length |v | = N . Return: an object Q. Complexity: O(N ). • Q.Argmin: Return the index to a minimal value of v . Complexity: O(1). • Q.Remove_Min: Remove the minimal element from the queue. Complexity: O(log N ). • Q.Update(i, x): Assign v [i] ← x and update the queue accordingly. Complexity: O(log N ). We can now describe the Generic_linkage algorithm step by step: Lines 5 to 8 search the nearest neighbor and the closest distance for each point x among all points y > x. This takes O(N 2 ) time. In line 9, we generate a priority queue from the list of nearest neighbors and minimal distances. The main loop is from line 10 to the end of the algorithm. In each step, the list L for a stepwise dendrogram is extended by one row, in the same way as the primitive clustering algorithm does. Lines 11 to 20 ﬁnd a current pair of closest nodes. A candidate for this is the minimal index in the queue (assigned to a), and its candidate for the nearest neighbor b := n _nghbr [a]. If the lower bound mindist [a] is equal to the actual dissimilarity d[a, b], then we are sure that we have a pair of closest nodes and their distance. Otherwise, the candidates for the nearest neighbor and the minimal distance are not the true values, and we ﬁnd the true values for n _nghbr [a] and mindist [a] in line 15 in O(N ) time. We repeat this process and extract the minimum among all lower bounds from the queue until we ﬁnd a valid minimal entry in the queue and therefore the actual closest pair of points. This procedure is the performance bottleneck of the algorithm. The algorithm might be forced to update the nearest neighbor O(N ) times with an eﬀort of O(N ) for each of the O(N ) iterations, so the worst-case performance is bounded by O(N 3 ). In practice, the inner loop from lines 14 to 20 is executed less often, which results in faster performance. Lines 22 to 27 are nearly the same as in the primitive clustering algorithm. The only diﬀerence is that we specialize from an arbitrary label for the new node to re-using the index b for the joined node. The index a becomes invalid, and we replace any nearest-neighbor reference to a by a reference to the new cluster b in lines 28 to 32. Note that the array n _nghbr contains only candidates for the nearest neighbors, so we could have written any

10

valid index here; however, for the single linkage method, it makes sense to choose b: if the nearest neighbor of a node was at index a, it is now at b, which represents the join a ∪ b. The remaining code in the main loop ensures that the array n _nghbr still contains lower bounds on the distances to the nearest neighbors. If the distance from the new cluster x to a cluster b 